# Shell Sage Configuration
# ------------------------

# Operation Mode (local/api)
MODE=local  # Options: 'local' (Ollama) | 'api' (Cloud providers)
OLLAMA_HOST=http://localhost:11434

# Local Configuration
LOCAL_MODEL=llama3:8b-instruct-q4_1  # Ollama model name for local mode

# API Configuration
ACTIVE_API_PROVIDER=groq  # Current provider: groq, openai, anthropic, fireworks, openrouter, deepseek
API_MODEL=mixtral-8x7b-32768  # Provider-specific model name, you can add any model supported by your provider

# Provider API Keys (only set for your active provider)
GROQ_API_KEY=            # For Groq Cloud (https://console.groq.com)
OPENAI_API_KEY=          # For OpenAI (https://platform.openai.com)
ANTHROPIC_API_KEY=       # For Anthropic Claude (https://console.anthropic.com)
FIREWORKS_API_KEY=       # For Fireworks AI (https://app.fireworks.ai)
OPENROUTER_API_KEY=      # For OpenRouter (https://openrouter.ai)
DEEPSEEK_API_KEY=        # For Deepseek (https://platform.deepseek.com)