# Shell Sage Configuration
# ------------------------

# Operation Mode (local/api)
MODE=api  # Options: 'local' (Ollama) | 'api' (Cloud providers)
OLLAMA_HOST=http://localhost:11434

# Local Configuration
LOCAL_MODEL=llama3:8b-instruct-q4_1  # Ollama model name for local mode

# API Configuration
ACTIVE_API_PROVIDER=gemini  # Current provider: groq, openai, anthropic, fireworks, openrouter, deepseek
API_MODEL=gemini-2.0-flash-exp  # Provider-specific model name, you can add any model supported by your provider

# Provider API Keys (only set for your active provider)
GEMINI_API_KEY=
OPENAI_API_KEY=          # For OpenAI (https://platform.openai.com)
ANTHROPIC_API_KEY=       # For Anthropic Claude (https://console.anthropic.com)
FIREWORKS_API_KEY=       # For Fireworks AI (https://app.fireworks.ai)
OPENROUTER_API_KEY=      # For OpenRouter (https://openrouter.ai)
DEEPSEEK_API_KEY=        # For Deepseek (https://platform.deepseek.com)
